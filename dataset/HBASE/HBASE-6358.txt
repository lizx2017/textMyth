Summary:
Bulkloading from remote filesystem is problematic
Description:
Bulk loading hfiles that don't live on the same filesystem as HBase can cause problems for subtle reasons.
In Store.bulkLoadHFile(), the regionserver will copy the source hfile to its own filesystem if it's not already there. Since this can take a long time for large hfiles, it's likely that the client will timeout and retry. When the client retries repeatedly, there may be several bulkload operations in flight for the same hfile, causing lots of unnecessary IO and tying up handler threads. This can seriously impact performance. In my case, the cluster became unusable and the regionservers had to be kill -9'ed.
Possible solutions:
Require that hfiles already be on the same filesystem as HBase in order for bulkloading to succeed. The copy could be handled by LoadIncrementalHFiles before the regionserver is called.
Others? I'm not familiar with Hadoop IPC so there may be tricks to extend the timeout or something else.
I'm willing to write a patch but I'd appreciate recommendations on how to proceed.
Status:
RESOLVED
Priority:
Major
Resolution:
Incomplete
Affects_version:
0.94.0
Fix_version:
None
Component:
regionserver
Label:
None
Environment:

Attachment number:
0
Assignee:
Unassigned
Reporter:
Dave Revell
Create date:
09/Jul/12 21:50
Update date:
11/Apr/15 00:21
Resolved date:
11/Apr/15 00:21
