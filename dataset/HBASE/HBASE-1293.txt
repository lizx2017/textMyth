Summary:
hfile doesn't recycle decompressors
Description:
The Compression codec stuff from hadoop has the concept of recycling compressors and decompressors - this is because a compression codec uses "direct buffers" which reside outside the JVM regular heap space. There is a risk that under heavy concurrent load we could run out of that 'direct buffer' heap space in the JVM.
HFile does not call algorithm.returnDecompressor and returnCompressor. We should fix that.
I found this bug via OOM crashes under jdk 1.7 - it appears to be partially due to the size of my cluster (200gb, 800 regions, 19 servers) and partially due to weaknesses in JVM 1.7.
Status:
CLOSED
Priority:
Major
Resolution:
Fixed
Affects_version:
0.20.0
Fix_version:
0.20.0
Component:
None
Label:
None
Environment:

Attachment number:
0
Assignee:
ryan rawson
Reporter:
ryan rawson
Create date:
27/Mar/09 22:56
Update date:
13/Sep/09 22:24
Resolved date:
28/Mar/09 00:39
