Summary:
Duplicate tokens using WordDelimiterFilter for a specific configuration
Description:
When using both the options PRESERVE_ORIGINAL|SPLIT_ON_CASE_CHANGE|CONCATENATE_ALL using the WordDelimiterFilter, we have duplicate tokens on strings contaning only case changes.
When using the SPLIT_ON_CASE_CHANGE option, "abcDef" is split into "abc", "Def".
When having PRESERVE_ORIGINAL, we keep "abcDef".
However, when one uses CONCATENATE_ALL (or CATENATE_WORDS ?), it also adds another token built from the concatenation of the splited words, giving "abcDef" again.
I'm not 100% certain that token filters should not produce duplicate tokens (same word, same start and end positions). Can someone confirm this is a bug ?
Status:
RESOLVED
Priority:
Minor
Resolution:
Implemented
Affects_version:
None
Fix_version:
None
Component:
None
Label:
None
Environment:

Attachment number:
0
Assignee:
Unassigned
Reporter:
Jean-Baptiste Lespia
Create date:
27/Jan/16 21:40
Update date:
26/Apr/16 21:43
Resolved date:
26/Apr/16 21:37
