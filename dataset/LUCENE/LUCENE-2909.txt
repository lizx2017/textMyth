Summary:
NGramTokenFilter may generate offsets that exceed the length of original text
Description:
Whan using NGramTokenFilter combined with CharFilters that lengthen the original text (such as "ÃŸ" -> "ss"), the generated offsets exceed the length of the origianal text.
This causes InvalidTokenOffsetsException when you try to highlight the text in Solr.
While it is not possible to know the accurate offset of each character once you tokenize the whole text with tokenizers like KeywordTokenizer, NGramTokenFilter should at least avoid generating invalid offsets.
Status:
OPEN
Priority:
Minor
Resolution:
Unresolved
Affects_version:
2.9.4
Fix_version:
None
Component:
modules/analysis
Label:
None
Environment:

Attachment number:
0
Assignee:
Koji Sekiguchi
Reporter:
Shinya Kasatani
Create date:
07/Feb/11 05:14
Update date:
16/May/11 18:16
Resolved date:

