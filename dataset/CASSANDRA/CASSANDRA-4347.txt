Summary:
IP change of node requires assassinate to really remove old IP
Description:
In changing the IP addresses of nodes one-by-one, the node successfully moves itself and its token. Everything works properly.
However, the node which had its IP changed (but NOT other nodes in the ring) continues to have some type of state associated with the old IP and produces log messages like this:
INFO [GossipStage:1] 2012-06-15 15:25:01,490 Gossiper.java (line 838) Node /10.12.9.157 is now part of the cluster
INFO [GossipStage:1] 2012-06-15 15:25:01,490 Gossiper.java (line 804) InetAddress /10.12.9.157 is now UP
INFO [GossipStage:1] 2012-06-15 15:25:01,491 StorageService.java (line 1017) Nodes /10.12.9.157 and dev-cass01.sv.walmartlabs.com/10.93.15.11 have the same token 113427455640312821154458202477256070484. Ignoring /10.12.9.157
INFO [GossipTasks:1] 2012-06-15 15:25:11,373 Gossiper.java (line 818) InetAddress /10.12.9.157 is now dead.
INFO [GossipTasks:1] 2012-06-15 15:25:32,380 Gossiper.java (line 632) FatClient /10.12.9.157 has been silent for 30000ms, removing from gossip
INFO [GossipStage:1] 2012-06-15 15:26:32,490 Gossiper.java (line 838) Node /10.12.9.157 is now part of the cluster
INFO [GossipStage:1] 2012-06-15 15:26:32,491 Gossiper.java (line 804) InetAddress /10.12.9.157 is now UP
INFO [GossipStage:1] 2012-06-15 15:26:32,491 StorageService.java (line 1017) Nodes /10.12.9.157 and dev-cass01.sv.walmartlabs.com/10.93.15.11 have the same token 113427455640312821154458202477256070484. Ignoring /10.12.9.157
INFO [GossipTasks:1] 2012-06-15 15:26:42,402 Gossiper.java (line 818) InetAddress /10.12.9.157 is now dead.
INFO [GossipTasks:1] 2012-06-15 15:27:03,410 Gossiper.java (line 632) FatClient /10.12.9.157 has been silent for 30000ms, removing from gossip
INFO [GossipStage:1] 2012-06-15 15:28:04,533 Gossiper.java (line 838) Node /10.12.9.157 is now part of the cluster
Other nodes do NOT have the old IP showing up in logs. It's only the node that moved.
The old IP doesn't show up in ring anywhere or in any other fashion. The cluster seems to be fully operational, so I think it's just a cleanup issue.
Status:
RESOLVED
Priority:
Low
Resolution:
Won't Fix
Affects_version:

Fix_version:
None
Component:
None
Label:
None
Environment:
redhat
Attachment number:
0
Assignee:
Brandon Williams
Reporter:
Karl Mueller
Create date:
15/Jun/12 22:39
Update date:
16/Apr/19 09:32
Resolved date:
13/Nov/12 18:21
